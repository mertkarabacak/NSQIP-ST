{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc, precision_score, recall_score, f1_score, classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from scipy import stats as st\n",
    "from random import randrange\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open csv file.\n",
    "\n",
    "data = pd.read_csv(\"../input/st-cpt-only/st_cpt_only_final.csv\", index_col = 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data.columns)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define predictor variables and outcome of interest.\n",
    "\n",
    "variables = ['AGE', 'HEIGHT', 'WEIGHT', 'PRSODM', 'PRBUN', 'PRCREAT', 'PRWBC', 'PRHCT', 'PRPLATE', 'HTOODAY', 'BMI', 'SEX_female', 'SEX_male', 'OPERYR_2016', 'OPERYR_2017', 'OPERYR_2018', 'OPERYR_2019', 'OPERYR_2020', 'TRANST_Not transferred', 'TRANST_Transferred', 'TRANST_Unknown', 'SURGSPEC_Neurosurgery', 'SURGSPEC_Orthopedics', 'DIABETES_No', 'DIABETES_Yes', 'SMOKE_No', 'SMOKE_Yes', 'DYSPNEA_No', 'DYSPNEA_Yes', 'FNSTATUS2_Independent', 'FNSTATUS2_Partially Dependent', 'FNSTATUS2_Totally Dependent', 'FNSTATUS2_Unknown', 'HXCOPD_No', 'HXCOPD_Yes', 'ASCITES_No', 'HXCHF_No', 'HXCHF_Yes', 'HYPERMED_No', 'HYPERMED_Yes', 'RENAFAIL_No', 'DIALYSIS_No', 'DIALYSIS_Yes', 'DISCANCR_No', 'DISCANCR_Yes', 'WNDINF_No', 'WNDINF_Yes', 'STEROID_No', 'STEROID_Yes', 'WTLOSS_No', 'WTLOSS_Yes', 'BLEEDDIS_No', 'BLEEDDIS_Yes', 'TRANSFUS_No', 'TRANSFUS_Yes', 'ASACLAS_1-No Disturb', 'ASACLAS_2-Mild Disturb', 'ASACLAS_3-Severe Disturb', 'RACE_Asian', 'RACE_Black or African American', 'RACE_Hispanic', 'RACE_Other', 'RACE_Unknown', 'RACE_White', 'IEDUR_Extradural', 'IEDUR_Intradural', 'CPTx_63275', 'CPTx_63276', 'CPTx_63277', 'CPTx_63280', 'CPTx_63281', 'CPTx_63282', 'CPTx_63283', 'CPTx_63285', 'CPTx_63286', 'CPTx_63287', 'CPTx_63290', 'CPTx_63300', 'CPTx_63301', 'CPTx_63302', 'CPTx_63303', 'CPTx_63304', 'CPTx_63305', 'CPTx_63306', 'CPTx_63307', 'CPTx_Other', 'LOS_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine data.\n",
    "\n",
    "data = data[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define predictor variables (x) and outcome of interest (y).\n",
    "\n",
    "x = data.drop(['LOS_Yes'], axis = 1)\n",
    "y = data['LOS_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data shapes.\n",
    "\n",
    "print(y.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and test sets in 80:20 ratio.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "  \n",
    "#Describe train and test sets.\n",
    "\n",
    "print(\"Number patients x_train dataset: \", x_train.shape)\n",
    "print(\"Number patients y_train dataset: \", y_train.shape)\n",
    "print(\"Number patients x_test dataset: \", x_test.shape)\n",
    "print(\"Number patients y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe outcome of interest before resampling.\n",
    "\n",
    "print(\"Before resampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"Before resampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply ADASYN.\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "ada = ADASYN()\n",
    "x_rs, y_rs = ada.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe outcome of interest after resampling.\n",
    "\n",
    "print(\"After resampling, counts of label '1': {}\".format(sum(y_rs == 1)))\n",
    "print(\"After resampling, counts of label '0': {} \\n\".format(sum(y_rs == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for XGBoost.\n",
    "\n",
    "def objective(trial):\n",
    "    data, target = x_rs, y_rs\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\":  trial.suggest_categorical(\"objective\", [\"binary:logistic\"]),\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, evals=[(dvalid, \"validation\")], callbacks=[pruning_callback])\n",
    "    preds = bst.predict(dvalid)\n",
    "    pred_labels = np.rint(preds)\n",
    "    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n",
    "\n",
    "    return auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n",
    "    )\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "    xgb_params = {}\n",
    "    \n",
    "    for key, value in trial.params.items():\n",
    "        xgb_params[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit XGBoost.\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "\n",
    "xgb.fit(x_rs,y_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on the test set based on the trained XGBoost model.\n",
    "\n",
    "preds_xgb = xgb.predict(x_test)\n",
    "\n",
    "probs_xgb = xgb.predict_proba(x_test)\n",
    "probs_xgb = probs_xgb[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate XGBoost model.\n",
    "\n",
    "xgb_precision = precision_score(preds_xgb,y_test)\n",
    "xgb_recall = recall_score(preds_xgb,y_test)\n",
    "xgb_f1 = f1_score(preds_xgb,y_test)\n",
    "xgb_acc = accuracy_score(preds_xgb,y_test)   \n",
    "xgb_mcc = matthews_corrcoef(y_test, preds_xgb)\n",
    "xgb_auroc = roc_auc_score(y_test, probs_xgb)\n",
    "\n",
    "print(\"Precision: %.3f\" % (xgb_precision))\n",
    "print(\"Recall: %.3f\" % (xgb_recall))\n",
    "print(\"F1 Score: %.3f\" %(xgb_f1))\n",
    "print('Accuracy: %.3f' % (xgb_acc))\n",
    "print('MCC: %.3f' % (xgb_mcc))\n",
    "print('AUROC: %.3f' % (xgb_auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate XGBoost model (PRC and AUPRC).\n",
    "\n",
    "xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, probs_xgb)\n",
    "xgb_auprc = auc(xgb_recall, xgb_precision)\n",
    "\n",
    "print('AUPRC: %.3f' % (xgb_auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate precision and recall for calculation purposes.\n",
    "\n",
    "xgb_precision = precision_score(preds_xgb,y_test)\n",
    "xgb_recall = recall_score(preds_xgb,y_test)\n",
    "\n",
    "xgb_results = [xgb_precision, xgb_recall, xgb_f1, xgb_acc, xgb_mcc, xgb_auroc, xgb_auprc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate precision recall curve for plotting purposes.\n",
    "\n",
    "xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, probs_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for LightGBM.\n",
    "\n",
    "def objective(trial):\n",
    "    data, target = x_rs, y_rs\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "\n",
    "    param = {\n",
    "        \"objective\":  trial.suggest_categorical(\"objective\", [\"binary\"]),\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\":  trial.suggest_categorical(\"boosting_type\", [\"gbdt\"]),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(param, dtrain)\n",
    "    preds = gbm.predict(valid_x)\n",
    "    pred_labels = np.rint(preds)\n",
    "    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n",
    "    return auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "    lgb_params = {}\n",
    "    \n",
    "    for key, value in trial.params.items():\n",
    "        lgb_params[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit LightGBM.\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "lgb.fit(x_rs, y_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on the test set based on the trained model.\n",
    "\n",
    "preds_lgb = lgb.predict(x_test)\n",
    "\n",
    "probs_lgb = lgb.predict_proba(x_test)\n",
    "probs_lgb = probs_lgb[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate LightGBM model.\n",
    "\n",
    "lgb_precision = precision_score(preds_lgb,y_test)\n",
    "lgb_recall = recall_score(preds_lgb,y_test)\n",
    "lgb_f1 = f1_score(preds_lgb,y_test)\n",
    "lgb_acc = accuracy_score(preds_lgb,y_test)   \n",
    "lgb_mcc = matthews_corrcoef(y_test, preds_lgb)\n",
    "lgb_auroc = roc_auc_score(y_test, probs_lgb)\n",
    "                          \n",
    "print(\"Precision: %.3f\" % (lgb_precision))\n",
    "print(\"Recall: %.3f\" % (lgb_recall))\n",
    "print(\"F1 Score: %.3f\" %(lgb_f1))\n",
    "print('Accuracy: %.3f' % (lgb_acc))\n",
    "print('MCC: %.3f' % (lgb_mcc))\n",
    "print('AUROC: %.3f' % (lgb_auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate LightGBM model (PRC and AUPRC).\n",
    "\n",
    "lgb_precision, lgb_recall, _ = precision_recall_curve(y_test, probs_lgb)\n",
    "lgb_auprc = auc(lgb_recall, lgb_precision)\n",
    "\n",
    "print('AUPRC: %.3f' % (lgb_auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate precision and recall for calculation purposes.\n",
    "\n",
    "lgb_precision = precision_score(preds_lgb,y_test)\n",
    "lgb_recall = recall_score(preds_lgb,y_test)\n",
    "\n",
    "lgb_results = [lgb_precision, lgb_recall, lgb_f1, lgb_acc, lgb_mcc, lgb_auroc, lgb_auprc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate precision recall curve for plotting purposes.\n",
    "\n",
    "lgb_precision, lgb_recall, _ = precision_recall_curve(y_test, probs_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for CatBoost.\n",
    "\n",
    "from optuna.integration import CatBoostPruningCallback\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    data, target = x_rs, y_rs\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        \"used_ram_limit\": \"3gb\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "    }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n",
    "\n",
    "    gbm = cb.CatBoostClassifier(**param)\n",
    "\n",
    "    pruning_callback = CatBoostPruningCallback(trial, \"AUC\")\n",
    "    gbm.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        eval_set=[(valid_x, valid_y)],\n",
    "        verbose=0,\n",
    "        early_stopping_rounds=100,\n",
    "        callbacks=[pruning_callback],\n",
    "    )\n",
    "\n",
    "    pruning_callback.check_pruned()\n",
    "    preds = gbm.predict(valid_x)\n",
    "    pred_labels = np.rint(preds)\n",
    "    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n",
    "\n",
    "    return auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n",
    "    )\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    cb_params = {}\n",
    "    \n",
    "    for key, value in trial.params.items():\n",
    "        cb_params[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit CatBoost.\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cb = CatBoostClassifier(**cb_params)\n",
    "\n",
    "cb.fit(x_rs,y_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on the test set based on the trained model.\n",
    "\n",
    "preds_cb = cb.predict(x_test)\n",
    "\n",
    "probs_cb = cb.predict_proba(x_test)\n",
    "probs_cb = probs_cb[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate CatBoost model.\n",
    "\n",
    "cb_precision = precision_score(preds_cb,y_test)\n",
    "cb_recall = recall_score(preds_cb,y_test)\n",
    "cb_f1 = f1_score(preds_cb,y_test)\n",
    "cb_acc = accuracy_score(preds_cb,y_test)   \n",
    "cb_mcc = matthews_corrcoef(y_test, preds_cb)\n",
    "cb_auroc = roc_auc_score(y_test, probs_cb)\n",
    "                          \n",
    "print(\"Precision: %.3f\" % (cb_precision))\n",
    "print(\"Recall: %.3f\" % (cb_recall))\n",
    "print(\"F1 Score: %.3f\" %(cb_f1))\n",
    "print('Accuracy: %.3f' % (cb_acc))\n",
    "print('MCC: %.3f' % (cb_mcc))\n",
    "print('AUROC: %.3f' % (cb_auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate XGBoost model (PRC and AUPRC).\n",
    "\n",
    "cb_precision, cb_recall, _ = precision_recall_curve(y_test, probs_cb)\n",
    "cb_auprc = auc(cb_recall, cb_precision)\n",
    "\n",
    "print('AUPRC: %.3f' % (cb_auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate precision and recall for calculation purposes.\n",
    "\n",
    "cb_precision = precision_score(preds_cb,y_test)\n",
    "cb_recall = recall_score(preds_cb,y_test)\n",
    "\n",
    "cb_results = [cb_precision, cb_recall, cb_f1, cb_acc, cb_mcc, cb_auroc, cb_auprc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate precision recall curve for plotting purposes.\n",
    "\n",
    "cb_precision, cb_recall, _ = precision_recall_curve(y_test, probs_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for Random Forest.\n",
    "\n",
    "def objective(trial):\n",
    "    data, target = x_rs, y_rs\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)    \n",
    "    \n",
    "    param = {\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [\"auto\", \"sqrt\"]),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\",\"log2\", None]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 100),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000, 100),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4, 1),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10, 1),\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(**param)\n",
    "\n",
    "    rf.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "    )\n",
    "\n",
    "    preds = rf.predict(valid_x)\n",
    "    pred_labels = np.rint(preds)\n",
    "    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n",
    "\n",
    "    return auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "    rf_params = {}\n",
    "    \n",
    "    for key, value in trial.params.items():\n",
    "        rf_params[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Random Forest.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(**rf_params)\n",
    "\n",
    "rf.fit(x_rs,y_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on the test set based on the trained model.\n",
    "\n",
    "preds_rf = rf.predict(x_test)\n",
    "\n",
    "probs_rf = rf.predict_proba(x_test)\n",
    "probs_rf = probs_rf[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Random Forest model.\n",
    "\n",
    "rf_precision = precision_score(preds_rf,y_test)\n",
    "rf_recall = recall_score(preds_rf,y_test)\n",
    "rf_f1 = f1_score(preds_rf,y_test)\n",
    "rf_acc = accuracy_score(preds_rf,y_test)   \n",
    "rf_mcc = matthews_corrcoef(y_test, preds_rf)\n",
    "rf_auroc = roc_auc_score(y_test, probs_rf)\n",
    "                          \n",
    "print(\"Precision: %.3f\" % (rf_precision))\n",
    "print(\"Recall: %.3f\" % (rf_recall))\n",
    "print(\"F1 Score: %.3f\" %(rf_f1))\n",
    "print('Accuracy: %.3f' % (rf_acc))\n",
    "print('MCC: %.3f' % (rf_mcc))\n",
    "print('AUROC: %.3f' % (rf_auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Random Forest model (PRC and AUPRC).\n",
    "\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_test, probs_rf)\n",
    "rf_auprc = auc(rf_recall, rf_precision)\n",
    "\n",
    "print('AUPRC: %.3f' % (rf_auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate precision and recall for calculation purposes.\n",
    "\n",
    "rf_precision = precision_score(preds_rf,y_test)\n",
    "rf_recall = recall_score(preds_rf,y_test)\n",
    "\n",
    "rf_results = [rf_precision, rf_recall, rf_f1, rf_acc, rf_mcc, rf_auroc, rf_auprc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate precision recall curve for plotting purposes.\n",
    "\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_test, probs_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC, PR, and Calibration Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pyplot.figure()\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(12)\n",
    "\n",
    "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, probs_xgb)\n",
    "pyplot.plot(xgb_fpr, xgb_tpr, label='XGBoost AUROC: {:.3f}'.format(xgb_auroc), color='red')\n",
    "\n",
    "lgb_fpr, lgb_tpr, _ = roc_curve(y_test, probs_lgb)\n",
    "pyplot.plot(lgb_fpr, lgb_tpr, label='LightGBM AUROC: {:.3f}'.format(lgb_auroc), color='darkblue')\n",
    "\n",
    "cb_fpr, cb_tpr, _ = roc_curve(y_test, probs_cb)\n",
    "pyplot.plot(cb_fpr, cb_tpr, label='CatBoost AUROC: {:.3f}'.format(cb_auroc), color = 'darkgreen')\n",
    "\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, probs_rf)\n",
    "pyplot.plot(rf_fpr, rf_tpr, label='Random Forest AUROC: {:.3f}'.format(rf_auroc), color = 'orange')\n",
    "\n",
    "pyplot.plot([0, 1], [0, 1], linestyle = '--')\n",
    "\n",
    "pyplot.title('Receiver Operating Characteristic Curve', loc='center', fontsize = 20, fontweight = 'heavy', pad = 20)\n",
    "pyplot.xlabel('False Positive Rate', fontsize = 16, labelpad = 10)\n",
    "pyplot.ylabel('True Positive Rate', fontsize = 16, labelpad = 10)\n",
    "pyplot.tick_params(axis=\"y\",direction=\"out\")\n",
    "pyplot.tick_params(axis=\"x\",direction=\"out\")\n",
    "leg = pyplot.legend(loc = 'lower right', fontsize = 12)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pyplot.figure()\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(12)\n",
    "\n",
    "pyplot.plot(xgb_recall, xgb_precision, label='XGBoost AUPRC: {:.3f}'.format(xgb_auprc), color = 'red')\n",
    "pyplot.plot(lgb_recall, lgb_precision, label='LightGBM AUPRC: {:.3f}'.format(lgb_auprc), color = 'darkblue')\n",
    "pyplot.plot(cb_recall, cb_precision, label='CatBoost AUPRC: {:.3f}'.format(cb_auprc), color = 'darkgreen')\n",
    "pyplot.plot(rf_recall, rf_precision, label='Random Forest AUPRC: {:.3f}'.format(rf_auprc), color = 'orange')\n",
    "\n",
    "\n",
    "pyplot.title('Precision Recall Curve', loc='center', fontsize = 20, fontweight = 'heavy', pad = 20)\n",
    "pyplot.xlabel('Recall', fontsize = 16, labelpad = 10)\n",
    "pyplot.ylabel('Precision', fontsize = 16, labelpad = 10)\n",
    "leg = pyplot.legend(loc = 'lower right', fontsize = 12)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pyplot.figure()\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(12)\n",
    "\n",
    "x_cal_xgb, y_cal_xgb = calibration_curve(y_test, probs_xgb, n_bins = 10, normalize = True)\n",
    "x_cal_lgb, y_cal_lgb = calibration_curve(y_test, probs_lgb, n_bins = 10, normalize = True)\n",
    "x_cal_cb, y_cal_cb = calibration_curve(y_test, probs_cb, n_bins = 10, normalize = True)\n",
    "x_cal_rf, y_cal_rf = calibration_curve(y_test, probs_rf, n_bins = 10, normalize = True)\n",
    "\n",
    "\n",
    "pyplot.plot([0, 1], [0, 1], linestyle = '--', label = 'Ideally Calibrated')\n",
    "\n",
    "pyplot.plot(y_cal_xgb, x_cal_xgb, label = 'XGBoost', color = 'red')\n",
    "pyplot.plot(y_cal_lgb, x_cal_lgb, label = 'LightGBM', color = 'darkblue')\n",
    "pyplot.plot(y_cal_cb, x_cal_cb, label = 'CatBoost', color = 'darkgreen')\n",
    "pyplot.plot(y_cal_rf, x_cal_xgb, label = 'Random Forest', color = 'orange')\n",
    "\n",
    "pyplot.title('Calibration', loc='center', fontsize = 20, fontweight = 'heavy', pad = 20)\n",
    "leg = pyplot.legend(loc = 'lower right', fontsize = 12)\n",
    "pyplot.xlabel('Average Predicted Probability in each bin', fontsize = 16, labelpad = 10)\n",
    "pyplot.ylabel('Ratio of positives', fontsize = 16, labelpad = 10)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'XGBoost':xgb_results, 'LightGBM':lgb_results, 'CatBoost':cb_results, 'Random Forest':rf_results}\n",
    "\n",
    "results = pd.DataFrame(results, columns = ['XGBoost', 'LightGBM', 'CatBoost', 'Random Forest'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'XGBoost':xgb_results, 'LightGBM':lgb_results, 'CatBoost':cb_results, 'Random Forest': rf_results})\n",
    "\n",
    "results = results.T\n",
    "\n",
    "results.columns = ['Precision', 'Recall', 'F1', 'Accuracy', 'MCC', 'AUROC', 'AUPRC']\n",
    "\n",
    "results.to_csv('st_cpt_only_los_results.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def print_feature_importances_shap_values(shap_values, features):\n",
    "    '''\n",
    "    Prints the feature importances based on SHAP values in an ordered way\n",
    "    shap_values -> The SHAP values calculated from a shap.Explainer object\n",
    "    features -> The name of the features, on the order presented to the explainer\n",
    "    '''\n",
    "    \n",
    "    # Calculates the feature importance (mean absolute shap value) for each feature.\n",
    "    importances = []\n",
    "    for i in range(shap_values.values.shape[1]):\n",
    "        importances.append(np.mean(np.abs(shap_values.values[:, i])))\n",
    "        \n",
    "    # Calculates the normalized version.\n",
    "    importances_norm = softmax(importances)\n",
    "    \n",
    "    # Organize the importances and columns in a dictionary.\n",
    "    feature_importances = {fea: imp for imp, fea in zip(importances, features)}\n",
    "    feature_importances_norm = {fea: imp for imp, fea in zip(importances_norm, features)}\n",
    "    \n",
    "    # Sorts the dictionary.\n",
    "    feature_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1], reverse = True)}\n",
    "    feature_importances_norm= {k: v for k, v in sorted(feature_importances_norm.items(), key=lambda item: item[1], reverse = True)}\n",
    "    \n",
    "    # Prints the feature importances.\n",
    "    for k, v in feature_importances.items():\n",
    "        print(f\"{k} -> {v:.4f} (softmax = {feature_importances_norm[k]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_explainer = shap.Explainer(xgb.predict, x_test)\n",
    "lgb_explainer = shap.Explainer(lgb.predict, x_test)\n",
    "cb_explainer = shap.Explainer(cb.predict, x_test)\n",
    "rf_explainer = shap.Explainer(rf.predict, x_test)\n",
    "\n",
    "xgb_shap_values = xgb_explainer(x_test)\n",
    "lgb_shap_values = lgb_explainer(x_test)\n",
    "cb_shap_values = cb_explainer(x_test)\n",
    "rf_shap_values = rf_explainer(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(xgb_shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(lgb_shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(cb_shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(rf_shap_values, max_display = 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
